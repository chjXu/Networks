# 1.介绍一下dropout和normalization以及他们在训练和预测时的区别，如何解决batch normalization在训练和预测时统计量不一致的问题

  dropout是在alexNet工作提出的，其用于卷积之后，使一部分神经元失活，目的是防止网络过拟合，使网络泛化能力更强。
  normalization是把特征的各个维度标准化到特定区间，其优势为：加快网络训练，提升模型精度
  
  训练和测试的区别：
  
  （1）dropout. 训练时会随机丢弃一些神经元。在测试时往往关闭dropout，目的是保证预测结果的一致性，不关闭可能同一个输入会得到不同的输出。
  为了对齐Dropout训练和预测的结果，通常有两种做法，假设dropout rate = 0.2。一种是训练时不做处理，预测时输出乘以(1 - dropout rate)。
  另一种是训练时留下的神经元除以(1 - dropout rate)，预测时不做处理。
  
  （2）BN在训练时是在每个batch上计算均值和方差来进行归一化，每个batch的样本量都不大，所以每次计算出来的均值和方差就存在差异。
  预测时一般传入一个样本，所以不存在归一化。
  
  通常是通过滑动平均结合训练时所有batch的均值和方差来得到一个总体均值和方差


# 2. L1正则化与L2正则化的区别
  ## L1减少的是一个常量，L2减少的是权重的固定比例
  ## L1使权重稀疏，L2使权重平滑
  ## L1优点是能够获得sparse模型，对于large-scale的问题来说这一点很重要，因为可以减少存储空间
  ## L2优点是实现简单，能够起到正则化的作用。缺点就是L1的优点：无法获得sparse模型
