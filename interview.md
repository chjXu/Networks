# 1.介绍一下dropout和normalization以及他们在训练和预测时的区别，如何解决batch normalization在训练和预测时统计量不一致的问题

  dropout是在alexNet工作提出的，其用于卷积之后，使一部分神经元失活，目的是防止网络过拟合，使网络泛化能力更强。
  normalization是把特征的各个维度标准化到特定区间，其优势为：加快网络训练，提升模型精度
  
  训练和测试的区别：
  
  （1）dropout. 训练时会随机丢弃一些神经元，在测试时往往关闭dropout，保证预测结果的一致性，不关闭可能同一个输入会得到不同的输出。
  为了对齐Dropout训练和预测的结果，通常有两种做法，假设dropout rate = 0.2。一种是训练时不做处理，预测时输出乘以(1 - dropout rate)。
  另一种是训练时留下的神经元除以(1 - dropout rate)，预测时不做处理。
  
  （2）BN在训练时是在每个batch上计算均值和方差来进行归一化，每个batch的样本量都不大，所以每次计算出来的均值和方差就存在差异。
  预测时一般传入一个样本，所以不存在归一化。
  
  通常是通过滑动平均结合训练时所有batch的均值和方差来得到一个总体均值和方差
